{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10488806,"sourceType":"datasetVersion","datasetId":6494155},{"sourceId":10466143,"sourceType":"datasetVersion","datasetId":6479960}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain_community\n!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:42:48.673388Z","iopub.execute_input":"2025-02-15T05:42:48.673687Z","iopub.status.idle":"2025-02-15T05:43:08.060750Z","shell.execute_reply.started":"2025-02-15T05:42:48.673661Z","shell.execute_reply":"2025-02-15T05:43:08.059455Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.2/413.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfplumber\n  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\nDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import spacy\nimport re\nimport pandas as pd\nfrom langchain_community.document_loaders import PDFPlumberLoader\nfrom spacy.matcher import Matcher","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:08.688306Z","iopub.execute_input":"2025-02-15T05:47:08.688883Z","iopub.status.idle":"2025-02-15T05:47:16.743947Z","shell.execute_reply.started":"2025-02-15T05:47:08.688833Z","shell.execute_reply":"2025-02-15T05:47:16.743020Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def extract_text_pdf(pdf_path):\n        loader = PDFPlumberLoader(pdf_path)\n        docs = loader.load()\n        text = ''\n        for doc in docs:\n            text += doc.page_content # Append extracted text\n        return text \n\n\ndef extract_name(nlp_text, matcher):\n    '''\n    Helper function to extract name from spacy nlp text\n\n    :param nlp_text: object of `spacy.tokens.doc.Doc`\n    :param matcher: object of `spacy.matcher.Matcher`\n    :return: string of full name\n    '''\n    pattern =  [{'POS': 'PROPN'}, {'POS': 'PROPN'}] \n    \"\"\"extrcted particular pattern variable\n        rather than imporing from a large data\n        \"\"\"\n    \n    matcher.add(\"NAME\", [pattern])# changed from \"matcher.add('NAME', None, *pattern)\"\n    \n    matches = matcher(nlp_text)\n    \n    for match_id, start, end in matches:\n        span = nlp_text[start:end]\n        return span.text\n\ndef extract_email(resume_text):\n    '''\n    Helper function to extract email id from text\n\n    :param text: plain text extracted from resume file\n    '''\n    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", resume_text)\n    if email:\n        try:\n            return email[0].split()[0].strip(';')\n        except IndexError:\n            return None\n\n\n\ndef extract_education_spacy(resume_text):\n    \"\"\"\n    Extracts the Education section from resume text using spaCy.\n    \n    :param resume_text: Plain resume text\n    :return: Extracted \"Education\" section as a string\n    \"\"\"\n\n    # Split the resume into lines\n    lines = resume_text.split(\"\\n\")\n\n    education_section = []\n    \n    # Extract lines after \"EDUCATION\" until the next section\n    for line in lines:\n        if education_section and re.search(r\"\\b(project|skills|work experience|certifications)\\b\", line, re.IGNORECASE):  \n            break \n    \n        if re.search(r\"\\b(education)\\b\", line, re.IGNORECASE) or education_section:  \n            education_section.append(line.strip())\n    \n    education_text = \"\\n\".join(education_section)\n\n    education_text = education_text.replace('\\\\n', ' ').replace('\\n', ' ').strip()\n\n    return education_text\n\n\n\n\ndef extract_skills(nlp_text, noun_chunks):\n    '''\n    Helper function to extract skills from spacy nlp text\n\n    :param nlp_text: object of `spacy.tokens.doc.Doc`\n    :param noun_chunks: noun chunks extracted from nlp text\n    :return: list of skills extracted\n    '''\n    tokens = [token.text for token in nlp_text if not token.is_stop]\n    \n    # if noun_chunks is None:\n    #     noun_chunks = list(nlp_text.noun_chunks)\n\n    \n    url = \"https://raw.githubusercontent.com/OmkarPathak/ResumeParser/refs/heads/master/resume_parser/resume_parser/skills.csv\"\n    df = pd.read_csv(url, index_col=0)\n    data = df\n    \n    skills = list(data.columns.values)\n    \n    skillset = []\n    # check for one-grams\n    for token in tokens:\n        if token.lower() in skills:\n            skillset.append(token)\n    \n    # check for bi-grams and tri-grams\n    for token in noun_chunks:\n        token = token.text.lower().strip()\n        if token in skills:\n            skillset.append(token)\n    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n\n\ndef extract_experience_spacy(resume_text):\n    '''\n    Extracts the experience section from resume text using spaCy.\n    Also extracts job titles, company names, and dates.\n    \n    :param resume_text: Plain resume text\n    :return: Extracted \"Experience\" section as a string\n    '''\n\n\n    # Step 1: Extract the \"Experience\" section based on headers\n    lines = resume_text.split(\"\\n\")\n    experience_section = []\n    capture = False\n\n   # Step 1: Start capturing after \"WORK EXPERIENCE\" or similar headers\n    for line in lines:\n        if re.search(r\"\\b(experience|work experience|employment history)\\b\", line, re.IGNORECASE):  # Detect section start\n            capture = True\n        elif capture and re.search(r\"\\b(education|skills|projects)\\b\", line, re.IGNORECASE):  # Stop at next section\n            break\n        if capture:\n            experience_section.append(line.strip())\n\n    experience_text = \"\\n\".join(experience_section)\n    experience_text = experience_text.replace('\\\\n', ' ').replace('\\n', ' ').strip()\n\n    # Step 2: Extract structured entities using spaCy\n    experience_doc = nlp(experience_text)\n    extracted_info = {\n        \"Job Titles\": [],\n        \"Companies\": [],\n        \"Dates\": []\n    }\n    return experience_text\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:22.628379Z","iopub.execute_input":"2025-02-15T05:47:22.629343Z","iopub.status.idle":"2025-02-15T05:47:22.644639Z","shell.execute_reply.started":"2025-02-15T05:47:22.629300Z","shell.execute_reply":"2025-02-15T05:47:22.643575Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/steve-resume3/Steve Sun - Resume (2).pdf\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:28.347986Z","iopub.execute_input":"2025-02-15T05:47:28.348337Z","iopub.status.idle":"2025-02-15T05:47:28.352860Z","shell.execute_reply.started":"2025-02-15T05:47:28.348307Z","shell.execute_reply":"2025-02-15T05:47:28.351620Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"text = extract_text_pdf(pdf_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:30.087685Z","iopub.execute_input":"2025-02-15T05:47:30.088036Z","iopub.status.idle":"2025-02-15T05:47:30.403452Z","shell.execute_reply.started":"2025-02-15T05:47:30.088010Z","shell.execute_reply":"2025-02-15T05:47:30.402356Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\nnlp_text = nlp(text)\nnoun_chunks = list(nlp_text.noun_chunks)\nmatcher = Matcher(nlp.vocab)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:31.927798Z","iopub.execute_input":"2025-02-15T05:47:31.928369Z","iopub.status.idle":"2025-02-15T05:47:33.013682Z","shell.execute_reply.started":"2025-02-15T05:47:31.928336Z","shell.execute_reply":"2025-02-15T05:47:33.012710Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"output = {\n    \"Name\": extract_name(nlp_text, matcher),\n    \"Email\": extract_email(text),\n    \"Skills\":extract_skills(nlp_text, noun_chunks),\n    \"Education\": extract_education_spacy(text),\n    \"Experience\": extract_experience_spacy(text),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:35.007709Z","iopub.execute_input":"2025-02-15T05:47:35.008059Z","iopub.status.idle":"2025-02-15T05:47:35.446589Z","shell.execute_reply.started":"2025-02-15T05:47:35.008030Z","shell.execute_reply":"2025-02-15T05:47:35.445551Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:38.667980Z","iopub.execute_input":"2025-02-15T05:47:38.668318Z","iopub.status.idle":"2025-02-15T05:47:38.673118Z","shell.execute_reply.started":"2025-02-15T05:47:38.668292Z","shell.execute_reply":"2025-02-15T05:47:38.671929Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"with open('resume_SpaCy.json', 'w') as json_file:\n    json.dump(output, json_file, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:47:41.588586Z","iopub.execute_input":"2025-02-15T05:47:41.588932Z","iopub.status.idle":"2025-02-15T05:47:41.594263Z","shell.execute_reply.started":"2025-02-15T05:47:41.588902Z","shell.execute_reply":"2025-02-15T05:47:41.593120Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create the file\nwith open('resume_SpaCy.json', 'w') as json_file:\n    json.dump(output, json_file, indent=4)\n\n# Create a downloadable link\nFileLink('resume_SpaCy.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:48:32.088962Z","iopub.execute_input":"2025-02-15T05:48:32.089344Z","iopub.status.idle":"2025-02-15T05:48:32.098120Z","shell.execute_reply.started":"2025-02-15T05:48:32.089317Z","shell.execute_reply":"2025-02-15T05:48:32.097128Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/resume_SpaCy.json","text/html":"<a href='resume_SpaCy.json' target='_blank'>resume_SpaCy.json</a><br>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:51:12.829264Z","iopub.execute_input":"2025-02-15T05:51:12.829711Z","iopub.status.idle":"2025-02-15T05:51:12.835837Z","shell.execute_reply.started":"2025-02-15T05:51:12.829676Z","shell.execute_reply":"2025-02-15T05:51:12.834434Z"}},"outputs":[{"name":"stdout","text":"{'Name': 'Steve Sun', 'Email': 'stevesun1245@gmail.com', 'Skills': ['Engineering', 'Machine learning', 'Ai', 'Datasets', 'Communication', 'Modeling', 'Parser', 'Pdf', 'Process', 'Etl', 'Interactive', 'Workflows', 'Pandas', 'Data analysis', 'Kpis', 'Retention', 'Cloud', 'Python', 'Key performance indicators', 'Predictive analytics', 'Analytics', 'Sql', 'Testing', 'Analysis', 'Engagement'], 'Education': 'EDUCATION Skyline University Master of Science in Data Science - 3.81 GPA August 2018 – May 2020', 'Experience': 'WORK EXPERIENCE DataNova Insights Data Scientist March 2021 – Present • Developed and deployed machine learning models to optimize customer retention, increasing engagement by 15%. • Conducted exploratory data analysis (EDA) and statistical testing to derive actionable business insights. • Built scalable ETL pipelines to process large datasets using Python and SQL. • Collaborated with cross-functional teams to improve decision-making through predictive analytics. Neural Sphere Labs Data Scientist June 2019 – December 2023 • Designed and implemented deep learning models for image classification, improving accuracy by 20%. • Automated data preprocessing and feature engineering workflows using Python and Pandas. • Created interactive dashboards in Power BI to visualize key performance indicators (KPIs). • Partnered with engineers to deploy ML models into production, ensuring seamless integration.'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"for k,v in output.items():\n    print(k,v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T05:51:13.088230Z","iopub.execute_input":"2025-02-15T05:51:13.088816Z","iopub.status.idle":"2025-02-15T05:51:13.097351Z","shell.execute_reply.started":"2025-02-15T05:51:13.088771Z","shell.execute_reply":"2025-02-15T05:51:13.096162Z"}},"outputs":[{"name":"stdout","text":"Name Steve Sun\nEmail stevesun1245@gmail.com\nSkills ['Engineering', 'Machine learning', 'Ai', 'Datasets', 'Communication', 'Modeling', 'Parser', 'Pdf', 'Process', 'Etl', 'Interactive', 'Workflows', 'Pandas', 'Data analysis', 'Kpis', 'Retention', 'Cloud', 'Python', 'Key performance indicators', 'Predictive analytics', 'Analytics', 'Sql', 'Testing', 'Analysis', 'Engagement']\nEducation EDUCATION Skyline University Master of Science in Data Science - 3.81 GPA August 2018 – May 2020\nExperience WORK EXPERIENCE DataNova Insights Data Scientist March 2021 – Present • Developed and deployed machine learning models to optimize customer retention, increasing engagement by 15%. • Conducted exploratory data analysis (EDA) and statistical testing to derive actionable business insights. • Built scalable ETL pipelines to process large datasets using Python and SQL. • Collaborated with cross-functional teams to improve decision-making through predictive analytics. Neural Sphere Labs Data Scientist June 2019 – December 2023 • Designed and implemented deep learning models for image classification, improving accuracy by 20%. • Automated data preprocessing and feature engineering workflows using Python and Pandas. • Created interactive dashboards in Power BI to visualize key performance indicators (KPIs). • Partnered with engineers to deploy ML models into production, ensuring seamless integration.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}